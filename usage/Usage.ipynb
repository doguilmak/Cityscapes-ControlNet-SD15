{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n",
        "This notebook provides a comprehensive pipeline for controlled image generation using the Stable Diffusion model augmented with the ControlNet architecture. We focus on:\n",
        "\n",
        "* **Data Preparation**: Loading and preprocessing paired Cityscapes images and their semantic segmentation maps.\n",
        "\n",
        "* **Model Configuration**: Integrating the lllyasviel/sd-controlnet-seg ControlNet into a Stable Diffusion pipeline to condition image synthesis on segmentation masks.\n",
        "\n",
        "* **Training Loop**: Fine-tuning only the ControlNet branches while freezing the core Stable Diffusion components, using MSE loss on noise prediction across 50 epochs.\n",
        "\n",
        "* **Sampling & Inference**: Generating high-quality urban street scenes from unseen segmentation maps with a guidance scale of 9 and 50 diffusion steps.\n",
        "\n",
        "By the end of this notebook, you will have a fully trained ControlNet pipeline capable of producing realistic, segmentation-guided images. Let's get started!"
      ],
      "metadata": {
        "id": "GZ89qr21buzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure your runtime is **GPU** (_not_ CPU or TPU). And if it is an option, make sure you are using _Python 3_. You can select these settings by going to `Runtime -> Change runtime type -> Select the above mentioned settings and then press SAVE`.\n"
      ],
      "metadata": {
        "id": "NfkCuiinYYx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "fziO-FOVYlsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **0. Initial Steps**"
      ],
      "metadata": {
        "id": "MQBkGjtvb8RW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3jMIvXBQ_ta"
      },
      "outputs": [],
      "source": [
        "!git clone https://huggingface.co/doguilmak/cityscapes-controlnet-sd15"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from diffusers import StableDiffusionControlNetPipeline\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "9Cc8sPYyRnCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Loading the Pipeline**"
      ],
      "metadata": {
        "id": "JJDdPXLycBwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we run the following code we are loading a **Stable Diffusion pipeline with ControlNet** from a **local directory** (`/content/cityscapes-controlnet-sd15/full_pipeline`). This pipeline is designed to accept a conditioning input (such as a segmentation map) and generate an image based on a text prompt. The `safety_checker=None` disables the default NSFW content filter, which is often done when using custom or domain-specific models. The `torch_dtype=torch.float32` argument ensures the model uses 32-bit precision during inference, which is more compatible but slower than `float16`.\n",
        "\n",
        "Next, `torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")` checks if a GPU is available and assigns the appropriate device. Finally, `pipeline.to(device)` moves the entire pipeline (ControlNet, UNet, VAE, and other components) to the selected device, allowing inference to run on the GPU if available — which significantly improves speed."
      ],
      "metadata": {
        "id": "4oGULQFaVb72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    \"/content/cityscapes-controlnet-sd15/full_pipeline\",\n",
        "    safety_checker=None,\n",
        "    torch_dtype=torch.float32,\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pipeline.to(device)"
      ],
      "metadata": {
        "id": "pJa9Yy42Vf4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, calling `.eval()` on a model disables certain behaviors that are only needed during training, such as:\n",
        "- **Dropout layers**: These are turned off, so they don't randomly zero out activations.\n",
        "- **BatchNorm layers**: They use learned statistics (mean/variance) instead of computing them from the batch.\n",
        "\n",
        "Setting the model to evaluation mode ensures **deterministic and stable inference**, which is especially important when generating images. It also slightly improves performance and avoids subtle bugs that can happen if models remain in training mode during inference.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note: If you're using other submodules (like a `text_encoder` or `safety_checker`), you may also want to set those to `.eval()` if they are in use.**\n",
        "\n"
      ],
      "metadata": {
        "id": "EGjKaZAcUzbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.controlnet.eval()\n",
        "pipeline.unet.eval()\n",
        "pipeline.vae.eval()"
      ],
      "metadata": {
        "id": "KjGDqBPCRsFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `prompt`: The **positive text prompt** describing what you want to generate (e.g., a high-quality cityscape).\n",
        "- `neg_prompt`: The **negative prompt** helps steer the model **away** from undesired traits like blurriness or poor detail.\n",
        "- `num_steps`: Controls the number of diffusion steps — more steps can improve image quality but take more time.\n",
        "- `guidance`: Controls how strongly the model follows your text prompt. Values between **7 and 12** are common.\n",
        "\n",
        "You can pass this `config` dictionary into your generation function or pipeline call."
      ],
      "metadata": {
        "id": "hSXSD_OgV83l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Generate Images Using Fine-tuned Model**"
      ],
      "metadata": {
        "id": "mlgqQCIlF1Yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = 'street scene, cityscape, high-quality, detailed, HQ, 4K' # @param\n",
        "NEG_PROMPT = 'blurry, low quality, hazy' # @param\n",
        "NUM_STEPS = 50 # @param {type: \"number\"}\n",
        "GUIDANCE = 9 # @param {type: \"number\"}\n",
        "\n",
        "config = {\n",
        "    'prompt': PROMPT,\n",
        "    'neg_prompt': NEG_PROMPT,\n",
        "    'num_steps': NUM_STEPS,\n",
        "    'guidance': GUIDANCE\n",
        "}\n",
        "\n",
        "def load_control_image(path, size=(256, 256)):\n",
        "    \"\"\"\n",
        "    Loads and resizes a control image (e.g., a segmentation map) from the specified path.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the control image file.\n",
        "        size (tuple): Desired output size (width, height). Default is (256, 256).\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image: The resized RGB image.\n",
        "    \"\"\"\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    return img.resize(size)\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_from_path(image_path):\n",
        "    \"\"\"\n",
        "    Loads an image and generates an output image using the ControlNet pipeline.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image.\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image: The generated image based on the input and prompt.\n",
        "    \"\"\"\n",
        "    img = Image.open(image_path).convert(\"RGB\").resize((512, 512))\n",
        "    print(f\"DEBUG: image type={type(img)}, size={img.size}\")\n",
        "\n",
        "    out = pipeline(\n",
        "        prompt=[config[\"prompt\"]],\n",
        "        negative_prompt=[config[\"neg_prompt\"]],\n",
        "        image=img,\n",
        "        control_image=img,\n",
        "        num_inference_steps=config[\"num_steps\"],\n",
        "        guidance_scale=config[\"guidance\"],\n",
        "        output_type=\"pil\"\n",
        "    )\n",
        "\n",
        "    return out.images[0]\n",
        "\n",
        "def display_triplet(input_path, target_path):\n",
        "    \"\"\"\n",
        "    Loads an input image and a corresponding control image (annotation),\n",
        "    generates an output image using a predefined pipeline, and displays\n",
        "    all three side by side.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): Full path to the input image.\n",
        "        target_path (str): Full path to the target/control image.\n",
        "    \"\"\"\n",
        "    input_img = Image.open(input_path).convert(\"RGB\").resize((512, 512))\n",
        "    target_img = Image.open(target_path).convert(\"RGB\").resize((512, 512))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def gen(img, control):\n",
        "        \"\"\"\n",
        "        Generates an image from input and control using a pipeline.\n",
        "\n",
        "        Args:\n",
        "            img (PIL.Image): Input image.\n",
        "            control (PIL.Image): Control image (e.g., segmentation map).\n",
        "\n",
        "        Returns:\n",
        "            PIL.Image: Generated image.\n",
        "        \"\"\"\n",
        "        return pipeline(\n",
        "            prompt=[config[\"prompt\"]],\n",
        "            negative_prompt=[config[\"neg_prompt\"]],\n",
        "            image=img,\n",
        "            control_image=control,\n",
        "            num_inference_steps=config[\"num_steps\"],\n",
        "            guidance_scale=config[\"guidance\"],\n",
        "            output_type=\"pil\"\n",
        "        ).images[0]\n",
        "\n",
        "    generated_img = gen(input_img, target_img)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5), dpi=200)\n",
        "    for ax, im, title in zip(axes,\n",
        "                             [input_img, target_img, generated_img],\n",
        "                             [\"Input\", \"Target\", \"Generated\"]):\n",
        "        ax.imshow(im)\n",
        "        ax.set_title(title)\n",
        "        ax.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "c_c_KpCDRzYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = generate_from_path(\"/content/00002.jpg\")\n",
        "# display(result)\n",
        "plt.figure(figsize=(6, 6), dpi=200)\n",
        "plt.imshow(result)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yXoKPUMUW3Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_triplet(\"/content/00002_input.jpg\", \"/content/00002_annot.jpg\")"
      ],
      "metadata": {
        "id": "Z3eIMqyCW_Zm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}